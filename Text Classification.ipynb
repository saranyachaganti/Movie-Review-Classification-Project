{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Or Bad Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the inbuilt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if your numoy is 1.16.1 and below version use this \n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "# as the previous versions havefew inbuilt settings such as allow_pickle = True\n",
    "# its not the same for the later versions +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numpy versions above 1.16.1\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These numbers are integer encoded words each integer points to a certain word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1]) # both dont have different values as they are different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> i'm absolutely disgusted this movie isn't being sold all who love this movie should email disney and increase the demand for it they'd eventually have to sell it then i'd buy copies for everybody i know everything and everybody in this movie did a good job and i haven't figured out why disney hasn't put this movie on dvd or on vhs in rental stores at least i haven't seen any copies this is a wicked good movie and should be seen by all the kids in the new generation don't get to see it and i think they should it should at least be put back on the channel this movie doesn't deserve a cheap <UNK> it deserves the real thing i'm them now this movie will be on dvd\n"
     ]
    }
   ],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "_word_index = imdb.get_word_index()\n",
    "\n",
    "word_index = {k:(v+3) for k,v in _word_index.items()} # k=key, v=value\n",
    "word_index[\"<PAD>\"] = 0 #padding\n",
    "word_index[\"<START>\"] = 1 #start\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3 #unused\n",
    "\n",
    "# swap all data of keys ansd value\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "#print(reverse_word_index)\n",
    "\n",
    "# this function will return the decoded (human readable) reviews  \n",
    "def decode_review(text):\n",
    "\treturn \" \".join([reverse_word_index.get(i, \"?\") for i in text])\n",
    "print(decode_review(test_data[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> i generally love this type of movie however this time i found myself wanting to kick the screen since i can't do that i will just complain about it this was absolutely idiotic the things that happen with the dead kids are very cool but the alive people are absolute idiots i am a grown man pretty big and i can defend myself well however i would not do half the stuff the little girl does in this movie also the mother in this movie is reckless with her children to the point of neglect i wish i wasn't so angry about her and her actions because i would have otherwise enjoyed the flick what a number she was take my advise and fast forward through everything you see her do until the end also is anyone else getting sick of watching movies that are filmed so dark anymore one can hardly see what is being filmed as an audience we are <UNK> involved with the actions on the screen so then why the hell can't we have night vision\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(test_data[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad is displayed to set the value symetrically to all the other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 260\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data[0]), len(test_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To set padding as a fixed number accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], padding=\"post\", maxlen=250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], padding=\"post\", maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data[0]), len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          1408000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,408,289\n",
      "Trainable params: 1,408,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(88000, 16))  #generates word vectors and passes it to other layers \n",
    "model.add(keras.layers.GlobalAveragePooling1D()) # scaling the data i.e. put it in a lower dimension\n",
    "model.add(keras.layers.Dense(16, activation=\"relu\")) # identify patterns of words and classify \n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\")) # tewaking small disturbances and classifying\n",
    "\n",
    "model.summary()  # prints a summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "15000/15000 [==============================] - 2s 153us/sample - loss: 0.6918 - acc: 0.5715 - val_loss: 0.6897 - val_acc: 0.6272\n",
      "Epoch 2/80\n",
      "15000/15000 [==============================] - 2s 120us/sample - loss: 0.6857 - acc: 0.7073 - val_loss: 0.6814 - val_acc: 0.7318\n",
      "Epoch 3/80\n",
      "15000/15000 [==============================] - 2s 121us/sample - loss: 0.6727 - acc: 0.7469 - val_loss: 0.6649 - val_acc: 0.7558\n",
      "Epoch 4/80\n",
      "15000/15000 [==============================] - 2s 122us/sample - loss: 0.6497 - acc: 0.7589 - val_loss: 0.6387 - val_acc: 0.7595\n",
      "Epoch 5/80\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.6160 - acc: 0.7871 - val_loss: 0.6035 - val_acc: 0.7822\n",
      "Epoch 6/80\n",
      "15000/15000 [==============================] - 2s 132us/sample - loss: 0.5735 - acc: 0.8118 - val_loss: 0.5629 - val_acc: 0.7996\n",
      "Epoch 7/80\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.5265 - acc: 0.8262 - val_loss: 0.5188 - val_acc: 0.8186\n",
      "Epoch 8/80\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.4794 - acc: 0.8441 - val_loss: 0.4778 - val_acc: 0.8316\n",
      "Epoch 9/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.4351 - acc: 0.8595 - val_loss: 0.4405 - val_acc: 0.8410\n",
      "Epoch 10/80\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.3958 - acc: 0.8724 - val_loss: 0.4097 - val_acc: 0.8491\n",
      "Epoch 11/80\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.3626 - acc: 0.8808 - val_loss: 0.3843 - val_acc: 0.8576\n",
      "Epoch 12/80\n",
      "15000/15000 [==============================] - 2s 132us/sample - loss: 0.3348 - acc: 0.8881 - val_loss: 0.3651 - val_acc: 0.8611\n",
      "Epoch 13/80\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.3121 - acc: 0.8957 - val_loss: 0.3481 - val_acc: 0.8675\n",
      "Epoch 14/80\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.2917 - acc: 0.9003 - val_loss: 0.3357 - val_acc: 0.8710\n",
      "Epoch 15/80\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.2748 - acc: 0.9055 - val_loss: 0.3255 - val_acc: 0.8745\n",
      "Epoch 16/80\n",
      "15000/15000 [==============================] - 2s 131us/sample - loss: 0.2596 - acc: 0.9104 - val_loss: 0.3171 - val_acc: 0.8746\n",
      "Epoch 17/80\n",
      "15000/15000 [==============================] - 2s 129us/sample - loss: 0.2455 - acc: 0.9149 - val_loss: 0.3101 - val_acc: 0.8783\n",
      "Epoch 18/80\n",
      "15000/15000 [==============================] - 2s 131us/sample - loss: 0.2331 - acc: 0.9200 - val_loss: 0.3041 - val_acc: 0.8796\n",
      "Epoch 19/80\n",
      "15000/15000 [==============================] - 2s 132us/sample - loss: 0.2219 - acc: 0.9221 - val_loss: 0.2991 - val_acc: 0.8805\n",
      "Epoch 20/80\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.2117 - acc: 0.9261 - val_loss: 0.2957 - val_acc: 0.8825\n",
      "Epoch 21/80\n",
      "15000/15000 [==============================] - 2s 144us/sample - loss: 0.2015 - acc: 0.9308 - val_loss: 0.2929 - val_acc: 0.8829\n",
      "Epoch 22/80\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.1929 - acc: 0.9339 - val_loss: 0.2902 - val_acc: 0.8837\n",
      "Epoch 23/80\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.1841 - acc: 0.9387 - val_loss: 0.2893 - val_acc: 0.8846\n",
      "Epoch 24/80\n",
      "15000/15000 [==============================] - 2s 129us/sample - loss: 0.1766 - acc: 0.9425 - val_loss: 0.2877 - val_acc: 0.8844\n",
      "Epoch 25/80\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.1688 - acc: 0.9451 - val_loss: 0.2861 - val_acc: 0.8848\n",
      "Epoch 26/80\n",
      "15000/15000 [==============================] - 2s 128us/sample - loss: 0.1620 - acc: 0.9483 - val_loss: 0.2868 - val_acc: 0.8844\n",
      "Epoch 27/80\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.1554 - acc: 0.9509 - val_loss: 0.2862 - val_acc: 0.8856\n",
      "Epoch 28/80\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.1491 - acc: 0.9540 - val_loss: 0.2868 - val_acc: 0.8851\n",
      "Epoch 29/80\n",
      "15000/15000 [==============================] - 2s 149us/sample - loss: 0.1437 - acc: 0.9564 - val_loss: 0.2885 - val_acc: 0.8847\n",
      "Epoch 30/80\n",
      "15000/15000 [==============================] - 2s 129us/sample - loss: 0.1380 - acc: 0.9581 - val_loss: 0.2878 - val_acc: 0.8870\n",
      "Epoch 31/80\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.1321 - acc: 0.9613 - val_loss: 0.2889 - val_acc: 0.8867\n",
      "Epoch 32/80\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.1269 - acc: 0.9635 - val_loss: 0.2904 - val_acc: 0.8854\n",
      "Epoch 33/80\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.1218 - acc: 0.9659 - val_loss: 0.2927 - val_acc: 0.8865\n",
      "Epoch 34/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.1173 - acc: 0.9665 - val_loss: 0.2950 - val_acc: 0.8865\n",
      "Epoch 35/80\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.1131 - acc: 0.9677 - val_loss: 0.2972 - val_acc: 0.8858\n",
      "Epoch 36/80\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.1086 - acc: 0.9696 - val_loss: 0.2992 - val_acc: 0.8847\n",
      "Epoch 37/80\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.1042 - acc: 0.9714 - val_loss: 0.3020 - val_acc: 0.8842\n",
      "Epoch 38/80\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.1002 - acc: 0.9730 - val_loss: 0.3056 - val_acc: 0.8838\n",
      "Epoch 39/80\n",
      "15000/15000 [==============================] - 2s 129us/sample - loss: 0.0970 - acc: 0.9738 - val_loss: 0.3094 - val_acc: 0.8833\n",
      "Epoch 40/80\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.0929 - acc: 0.9761 - val_loss: 0.3122 - val_acc: 0.8845\n",
      "Epoch 41/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0894 - acc: 0.9777 - val_loss: 0.3145 - val_acc: 0.8824\n",
      "Epoch 42/80\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.0855 - acc: 0.9795 - val_loss: 0.3189 - val_acc: 0.8832\n",
      "Epoch 43/80\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.0823 - acc: 0.9798 - val_loss: 0.3218 - val_acc: 0.8828\n",
      "Epoch 44/80\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.0792 - acc: 0.9821 - val_loss: 0.3265 - val_acc: 0.8810\n",
      "Epoch 45/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0763 - acc: 0.9825 - val_loss: 0.3297 - val_acc: 0.8810\n",
      "Epoch 46/80\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 0.0730 - acc: 0.9837 - val_loss: 0.3358 - val_acc: 0.8774\n",
      "Epoch 47/80\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.0705 - acc: 0.9841 - val_loss: 0.3384 - val_acc: 0.8787\n",
      "Epoch 48/80\n",
      "15000/15000 [==============================] - 2s 129us/sample - loss: 0.0674 - acc: 0.9849 - val_loss: 0.3421 - val_acc: 0.8795\n",
      "Epoch 49/80\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.0650 - acc: 0.9860 - val_loss: 0.3477 - val_acc: 0.8795\n",
      "Epoch 50/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0622 - acc: 0.9869 - val_loss: 0.3513 - val_acc: 0.8778\n",
      "Epoch 51/80\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.0597 - acc: 0.9876 - val_loss: 0.3566 - val_acc: 0.8781\n",
      "Epoch 52/80\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.0575 - acc: 0.9885 - val_loss: 0.3644 - val_acc: 0.8737\n",
      "Epoch 53/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0557 - acc: 0.9889 - val_loss: 0.3667 - val_acc: 0.8753\n",
      "Epoch 54/80\n",
      "15000/15000 [==============================] - 2s 147us/sample - loss: 0.0529 - acc: 0.9896 - val_loss: 0.3714 - val_acc: 0.8769\n",
      "Epoch 55/80\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.0509 - acc: 0.9908 - val_loss: 0.3759 - val_acc: 0.8750\n",
      "Epoch 56/80\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.0488 - acc: 0.9913 - val_loss: 0.3833 - val_acc: 0.8729\n",
      "Epoch 57/80\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.0466 - acc: 0.9917 - val_loss: 0.3866 - val_acc: 0.8748\n",
      "Epoch 58/80\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.0453 - acc: 0.9927 - val_loss: 0.3939 - val_acc: 0.8725\n",
      "Epoch 59/80\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.0431 - acc: 0.9931 - val_loss: 0.3972 - val_acc: 0.8735\n",
      "Epoch 60/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0414 - acc: 0.9933 - val_loss: 0.4027 - val_acc: 0.8728\n",
      "Epoch 61/80\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.0396 - acc: 0.9935 - val_loss: 0.4080 - val_acc: 0.8725\n",
      "Epoch 62/80\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.0380 - acc: 0.9940 - val_loss: 0.4132 - val_acc: 0.8720\n",
      "Epoch 63/80\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.0364 - acc: 0.9947 - val_loss: 0.4195 - val_acc: 0.8717\n",
      "Epoch 64/80\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.0350 - acc: 0.9949 - val_loss: 0.4246 - val_acc: 0.8708\n",
      "Epoch 65/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0336 - acc: 0.9950 - val_loss: 0.4333 - val_acc: 0.8708\n",
      "Epoch 66/80\n",
      "15000/15000 [==============================] - 2s 122us/sample - loss: 0.0327 - acc: 0.9953 - val_loss: 0.4365 - val_acc: 0.8693\n",
      "Epoch 67/80\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.0309 - acc: 0.9958 - val_loss: 0.4414 - val_acc: 0.8695\n",
      "Epoch 68/80\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.0296 - acc: 0.9961 - val_loss: 0.4477 - val_acc: 0.8691\n",
      "Epoch 69/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0288 - acc: 0.9965 - val_loss: 0.4541 - val_acc: 0.8682\n",
      "Epoch 70/80\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.0279 - acc: 0.9966 - val_loss: 0.4606 - val_acc: 0.8688\n",
      "Epoch 71/80\n",
      "15000/15000 [==============================] - 2s 146us/sample - loss: 0.0264 - acc: 0.9970 - val_loss: 0.4659 - val_acc: 0.8684\n",
      "Epoch 72/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0254 - acc: 0.9969 - val_loss: 0.4705 - val_acc: 0.8678\n",
      "Epoch 73/80\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.0244 - acc: 0.9971 - val_loss: 0.4769 - val_acc: 0.8679\n",
      "Epoch 74/80\n",
      "15000/15000 [==============================] - 2s 121us/sample - loss: 0.0233 - acc: 0.9975 - val_loss: 0.4813 - val_acc: 0.8670\n",
      "Epoch 75/80\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.0223 - acc: 0.9977 - val_loss: 0.4864 - val_acc: 0.8673\n",
      "Epoch 76/80\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.0216 - acc: 0.9979 - val_loss: 0.4936 - val_acc: 0.8674\n",
      "Epoch 77/80\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0206 - acc: 0.9981 - val_loss: 0.4995 - val_acc: 0.8667\n",
      "Epoch 78/80\n",
      "15000/15000 [==============================] - 2s 121us/sample - loss: 0.0199 - acc: 0.9981 - val_loss: 0.5038 - val_acc: 0.8668\n",
      "Epoch 79/80\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.0191 - acc: 0.9985 - val_loss: 0.5094 - val_acc: 0.8669\n",
      "Epoch 80/80\n",
      "15000/15000 [==============================] - 2s 131us/sample - loss: 0.0184 - acc: 0.9987 - val_loss: 0.5144 - val_acc: 0.8665\n",
      "25000/25000 [==============================] - 1s 34us/sample - loss: 0.5514 - acc: 0.8536\n",
      "[0.5514180936217308, 0.8536]\n"
     ]
    }
   ],
   "source": [
    "fitModel = model.fit(x_train, y_train, epochs=80, batch_size=512, validation_data=(x_val, y_val), verbose=1)\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy = 85.36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see a specific reviewand predict it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "<START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Prediction: [0.]\n",
      "Actual:0\n",
      "[0.5514180936217308, 0.8536]\n"
     ]
    }
   ],
   "source": [
    "test_review = test_data[0]\n",
    "predict = model.predict([test_review])\n",
    "print(\"Review: \")\n",
    "print(decode_review(test_review))\n",
    "print(\"Prediction: \" + str(predict[0]))\n",
    "print(\"Actual:\" + str(test_labels[0]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "<START> like some other people wrote i'm a die hard mario fan and i loved this game br br this game starts slightly boring but trust me it's worth it as soon as you start your hooked the levels are fun and <UNK> they will hook you <UNK> your mind turns to <UNK> i'm not kidding this game is also <UNK> and is beautifully done br br to keep this spoiler free i have to keep my mouth shut about details but please try this game it'll be worth it br br story 9 9 action 10 1 it's that good <UNK> 10 attention <UNK> 10 average 10 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Prediction: [0.99983716]\n",
      "Actual:1\n",
      "[0.5514180936217308, 0.8536]\n"
     ]
    }
   ],
   "source": [
    "test_review = test_data[4]\n",
    "predict = model.predict([test_review])\n",
    "print(\"Review: \")\n",
    "print(decode_review(test_review))\n",
    "print(\"Prediction: \" + str(predict[4]))\n",
    "print(\"Actual:\" + str(test_labels[4]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And test it on a outside data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_encode(s):\n",
    "    encoded = [1]\n",
    "\n",
    "    for word in s:\n",
    "        if word.lower() in word_index:\n",
    "            encoded.append(word_index[word.lower()])\n",
    "        else:\n",
    "            encoded.append(2)\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of all the animation classics from the Walt Disney Company, there is perhaps none that is more celebrated than \"The Lion King.\" Its acclaim is understandable: this is quite simply a glorious work of art.\"The Lion King\" gets off to a fantastic start. The film's opening number, \"The Circle of Life,\" is outstanding. The song lasts for about four minutes, but from the first sound, the audience is floored. Not even National Geographic can capture something this beautiful and dramatic. Not only is this easily the greatest moment in film animation, this is one of the greatest sequences in film history. The story that follows is not as majestic, but the film has to tell a story. Actually, the rest of the film holds up quite well. The story takes place in Africa, where the lions rule. Their king, Mufasa (James Earl Jones) has just been blessed with a son, Simba (Jonathan Taylor Thomas), who goes in front of his uncle Scar (Jeremy Irons) as next in line for the throne. Scar is furious, and sets in motion plans to usurp the throne for himself. After a tragedy occurs and Mufasa is killed, Scar persuades Simba to flee, leaving himself as king. Simba grows up in exile, but he learns that while he can run away from his past, he can never escape it. When viewing the film, it is obvious that \"The Lion King\" is quite different from its predecessors (and successors). This is an epic story that contains more dramatic power than all the other Disney films combined. While there are definitely some light-hearted moments, there is no denying the dark drama that takes up the bulk of the story. While it could be argued that Disney is the champion of family entertainment, this film is not for the very young. Some of the sequences are very dark and violent, many bordering on frightening, even for the older crowd.The voice actors are terrific. Jonathan Taylor Thomas brings a large dose of innocence to Young Simba. He's mischievous, but also terribly naive. His older counterpart, voiced by Matthew Broderick, equals him. He's older, but no less mature. The voices are so similar that it's almost impossible not to believe that they are the same character at different ages. Perhaps no one could have been better suited for the role of Mufasa than James Earl Jones. His baritone voice gives the Mufasa a quality of great power and wisdom; there is no question that his role is king. As Scar, Jeremy Irons is pitch-perfect. The drawing of the character is villainous, but Irons' vocal work complements the animation to create one of the most memorable, and vicious, villains in Disney history. He's unquestionably evil, but he's also clever, which makes him all the more dangerous. Manipulation, not violence is his greatest weapon. Providing some much needed comic relief are Nathan Lane and Ernie Sabella as Timon and Pumbaa, two other outcasts (a meerkat and a warthog), and Rowan Atkinson as Zazu. While there is definite fun from these characters, neither the actors nor the filmmakers allow them to go over-the-top and destroy the mood of the film.Disney's animated features are known for their gorgeous artwork. Nowhere is this more apparent than in \"The Lion King.\" Every single frame is jaw-dropping. The colors are rich, and the drawings are sharp and beautiful. One of the pitfalls of animation (both computer and hand-drawn) is that there is sometimes a visible distance between the subject and the background, making it seem as if the figure animation was cut and pasted on the background (this is obviously what happens, but it is up to the artists to make sure that it isn't noticeable). There is none of that here.Throughout the Golden Age of Disney animation, the films have been musicals. \"The Lion King\" is no different, and the songs are brilliant. All of the numbers are standouts (\"Can You Feel the Love Tonight\" won the Oscar, but in my opinion, \"The Circle of Life\" was better). In the cases of Simba and Nala (Simba's girlfriend), both young and old, there is a noticeable difference between the speaking and singing parts (everyone else does their own singing and speaking), but never mind. It still works, and that's what's important.\"The Lion King\" is not flawless, but on first viewing, they aren't noticeable, and it is likely that the young won't ever notice them. \"Beauty and the Beast\" was the first animated film to get an Oscar nomination for Best Picture (it lost to \"The Silence of the Lambs\"), and is thus far the only animated film to receive such an honor. That being the case, it's hard to understand why \"The Lion King\" was not given the same distinction. The two films are more or less equal in quality, and the nominees for the honor that year were not strong. If you haven't already, see \"The Lion King.\" You won't be disappointed.\n",
      "[[    6  4687  3744   200     4   875     5     4   978   231    12   306\n",
      "     17    48     4   822   748    16   605     5 12625    23     4   978\n",
      "     14     9   540    51   571    21    12     9    56     8     4  2719\n",
      "      8    97   252    15    12   218  6456    50     9   600     7    15\n",
      "      2     4  2050   559     7   910   748     4   108    28    77  2769\n",
      "      4  3082   711     9    57   275     5     4   690    26   530    32\n",
      "      7     4  1396    26 16015    70    25   235     4   119  4487  1199\n",
      "      4   735    21    11    61   652     4  4246     7   113    16   128\n",
      "     11     4  2936     7 10539     5 35738 20403   980   199   185     5\n",
      "    154    50     9     6  6456  1474   200     4  1386     5  1118   531\n",
      "    316   334   127    68   205  1118     5  1386    21   115   330    12\n",
      "    131   495     5   198   803     2  3082   711     9    24  3562    21\n",
      "     23    86   829    36   713  6456     5    12     9  1329    15     4\n",
      "    185   528   126  1495    98   936     5     4  2773    16     4    86\n",
      "   1125    22     8    79    35   735  4372    18   118   431    12   416\n",
      "      8     4  3542     7     4 10721     5     9  1346   230     4    64\n",
      "   1125    22     8  3911   141    35  2898    15   112     4   420    45\n",
      "    254     8   391   138     4  3082   711    16    24   348     4   172\n",
      "   8076     4   107   108    26    53    42   329  3215    11   489     5\n",
      "      4 13341    18     4  2898    15   291    71    24   565    48    25\n",
      "    774   460    67     4  3082   711    25   528    30   685]]\n",
      "[0.9385734]\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\", encoding=\"utf-8\") as f:\n",
    "\tfor line in f.readlines():\n",
    "\t\tnline = line.replace(\",\", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace(\"\\\"\",\"\").strip().split(\" \")\n",
    "\t\tencode = review_encode(nline)\n",
    "        # make the data 250 words long\n",
    "\t\tencode = keras.preprocessing.sequence.pad_sequences([encode], value=word_index[\"<PAD>\"], padding=\"post\", maxlen=250)\n",
    "\t\tpredict = model.predict(encode)\n",
    "\t\tprint(line)\n",
    "\t\tprint(encode)\n",
    "\t\tprint(predict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here our prediction is 93.85% POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
